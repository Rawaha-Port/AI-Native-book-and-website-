# باب 4: روبوٹکس میں AI اور مشین لرننگ

## 4.1 تعارف
پچھلے ابواب میں، ہم نے ایک ہیومنائڈ روبوٹ کے طبعی جسم اور اس کی حرکت کو کنٹرول کرنے والے کلاسیکی اصولوں کا جائزہ لیا۔ اب، ہم "دماغ" کو شامل کرتے ہیں۔ یہ باب مصنوعی ذہانت (AI) اور مشین لرننگ (ML) کے دائرے میں گہرائی میں جاتا ہے، وہ ٹیکنالوجیز جو روبوٹس کو اپنے ماحول کو سمجھنے، تجربے سے سیکھنے، اور ذہین فیصلے کرنے کی صلاحیت دیتی ہیں۔ ہم پہلے سے پروگرام شدہ ہدایات سے آگے بڑھ کر یہ جانیں گے کہ جدید روبوٹس غیر منظم، متحرک دنیاؤں میں کیسے موافقت اور کام کر سکتے ہیں۔ ہم یہ بھی دیکھیں گے کہ روبوٹس کمپیوٹر ویژن کا استعمال کرتے ہوئے کیسے "دیکھتے" ہیں، وہ کمک سیکھنے کے ذریعے پیچیدہ کاموں کو انجام دینا کیسے سیکھتے ہیں، اور وہ اعلیٰ سطحی اہداف حاصل کرنے کے لیے اپنے اعمال کی منصوبہ بندی کیسے کرتے ہیں۔

## 4.2 نظریاتی بنیادیں

### 4.2.1 ادراک: دنیا کا مطلب سمجھنا
ادراک حسی معلومات کی تشریح کرکے ماحول کا ایک اندرونی ماڈل بنانے کا عمل ہے۔ روبوٹس کے لیے، یہ تمام ذہین رویے کی بنیاد ہے۔

- **کمپیوٹر ویژن**: یہ شعبہ روبوٹس کو تصاویر اور ویڈیوز سے معلومات کی تشریح اور سمجھنے کے قابل بناتا ہے۔
    - **آبجیکٹ ڈیٹیکشن**: YOLO (You Only Look Once) اور R-CNN (Region-based Convolutional Neural Networks) جیسے الگورتھمز ایک روبوٹ کو ایک تصویر کے اندر متعدد اشیاء کی شناخت اور ان کا پتہ لگانے کی اجازت دیتے ہیں، ان کے گرد باؤنڈنگ بکس بنا کر۔
    - **سیمنٹک سیگمنٹیشن**: یہ آبجیکٹ ڈیٹیکشن سے ایک قدم آگے جاتا ہے اور تصویر میں ہر ایک پکسل کی درجہ بندی کرتا ہے۔ ایک روبوٹ کے لیے، اس کا مطلب ہے کہ وہ سڑک، فٹ پاتھ، دیگر پیدل چلنے والوں، اور آسمان کے درمیان فرق کر سکتا ہے، جو نیویگیشن کے لیے بہت اہم ہے۔
    - **3D تعمیر نو**: ایک یا زیادہ کیمروں سے ڈیٹا کا استعمال کرتے ہوئے، روبوٹس اشیاء یا مناظر کے 3D ماڈل بنا سکتے ہیں، جو پکڑنے اور ہیرا پھیری کے لیے ضروری ہے۔

- **پوائنٹ کلاؤڈ پروسیسنگ**:
    - **LiDAR کے ساتھ کام کرنا**: LiDAR سینسرز ایک "پوائنٹ کلاؤڈ" تیار کرتے ہیں، جو ماحول کی سطحوں کی نمائندگی کرنے والے 3D پوائنٹس کا ایک بھرپور سیٹ ہے۔ کیمرے کی تصاویر کے برعکس، پوائنٹ کلاؤڈز براہ راست اور درست فاصلے کی پیمائش فراہم کرتے ہیں۔
    - **SLAM (بیک وقت لوکلائزیشن اور میپنگ)**: یہ موبائل روبوٹکس میں سب سے بنیادی مسائل میں سے ایک ہے۔ SLAM الگورتھمز کا ایک طبقہ ہے جو ایک روبوٹ کو ایک نامعلوم ماحول کا نقشہ بنانے کی اجازت دیتا ہے جبکہ بیک وقت اس نقشے کے اندر اپنی پوزیشن پر نظر رکھتا ہے۔ یہ کسی بھی روبوٹ کے لیے بہت اہم ہے جسے خود مختار طور پر کام کرنے کی ضرورت ہے۔

### 4.2.2 کنٹرول کے لیے سیکھنا
جبکہ کلاسیکی کنٹرول (جیسے PID) عین مطابق، دہرائے جانے والے کاموں کے لیے بہترین ہے، مشین لرننگ روبوٹس کو نئی مہارتیں حاصل کرنے اور تبدیلیوں کے مطابق ڈھالنے کی اجازت دیتی ہے۔

- **نگرانی شدہ سیکھنا اور تقلید سیکھنا**:
    - **تقلید سیکھنا (یا مظاہرے سے سیکھنا)**: روبوٹ کو ایک نئی مہارت سکھانے کے سب سے سیدھے طریقوں میں سے ایک۔ ایک انسان روبوٹ کو ایک کام انجام دینے کے لیے ٹیلی آپریٹ کرتا ہے (جیسے دروازہ کھولنا)، اور روبوٹ اپنے جوڑوں کے زاویے اور سینسر ڈیٹا کو ریکارڈ کرتا ہے۔ پھر ایک ML ماڈل (جیسے نیورل نیٹ ورک) کو اس مظاہرہ شدہ رویے کی "تقلید" کرنے کے لیے تربیت دی جاتی ہے، سینسر ان پٹس کو موٹر کمانڈز پر میپ کرکے۔

- **کمک سیکھنا (RL)**:
    - **تصور**: RL ایک ایسا نمونہ ہے جہاں ایک ایجنٹ آزمائش اور غلطی کے ذریعے ایک کام انجام دینا سیکھتا ہے۔ ایجنٹ اپنے اعمال کے لیے "انعام" یا "سزا" وصول کرتا ہے اور اس کا مقصد ایک ایسی "پالیسی" (اعمال کو منتخب کرنے کی حکمت عملی) سیکھنا ہے جو وقت کے ساتھ اس کے مجموعی انعام کو زیادہ سے زیادہ کرے۔
    - **کلیدی اصطلاحات**:
        - **ایجنٹ**: سیکھنے والا یا فیصلہ کرنے والا (روبوٹ کا AI)۔
        - **ماحول**: وہ دنیا جس کے ساتھ ایجنٹ تعامل کرتا ہے۔
        - **حالت**: ایک خاص لمحے میں ماحول کا ایک سنیپ شاٹ۔
        - **عمل**: ایک حرکت جو ایجنٹ کر سکتا ہے۔
        - **انعام**: ایک فیڈ بیک سگنل جو یہ بتاتا ہے کہ ایک عمل کتنا اچھا تھا۔
        - **پالیسی**: حالتوں کو اعمال پر میپ کرنے کے لیے ایجنٹ کی حکمت عملی۔
    - **تلاش بمقابلہ استحصال**: RL میں ایک کلیدی چیلنج یہ ہے کہ نئے، نامعلوم اعمال کی تلاش کے درمیان توازن قائم کیا جائے تاکہ یہ دیکھا جا سکے کہ کیا وہ بہتر انعامات دیتے ہیں، بمقابلہ معلوم اعمال کا استحصال کرنا جو پہلے سے ہی اچھے انعامات دیتے ہیں۔

- **سم-ٹو-ریئل ٹرانسفر**:
    - حقیقی روبوٹس پر RL ایجنٹس کی تربیت سست، مہنگی، اور خطرناک ہو سکتی ہے۔لہذا، پالیسیوں کو ایک تیز، متوازی طبیعیات سمولیشن میں تربیت دینا عام ہے۔ تاہم، ایک "کامل" سمولیشن میں تربیت یافتہ پالیسی اکثر حقیقی روبوٹ پر طبیعیات، سینسر شور، اور موٹر ردعمل میں فرق کی وجہ سے ناکام ہو جاتی ہے۔
    - **سم-ٹو-ریئل** سمولیشن میں حاصل کردہ علم کو حقیقی دنیا میں منتقل کرنے کا چیلنج ہے۔ تکنیکوں میں **ڈومین رینڈمائزیشن** (پالیسی کو قدرے مختلف سمولیشنز کی ایک وسیع اقسام میں تربیت دینا) اور **سسٹم آئیڈینٹیفیکیشن** (حقیقی روبوٹ کی حرکیات کا ایک انتہائی درست ماڈل بنانا) شامل ہیں۔

### 4.2.3 منصوبہ بندی اور فیصلہ سازی
منصوبہ بندی میں ایک اعلیٰ سطحی ہدف کو نچلی سطح کے اعمال کے ایک سلسلے میں توڑنا شامل ہے۔

- **پاتھ پلاننگ الگورتھمز**:
    - **A***: ایک کلاسک گراف ٹراورسل اور پاتھ سرچ الگورتھم جو رکاوٹوں والے نقشے پر دو پوائنٹس کے درمیان مختصر ترین راستہ تلاش کرنے میں انتہائی موثر ہے۔
    - **RRT (ریپڈلی-ایکسپلورنگ رینڈم ٹریز)**: ایک الگورتھم جو خاص طور پر اعلیٰ جہتی جگہوں (جیسے ملٹی جوائنٹڈ روبوٹ بازو کی کنفیگریشن اسپیس) میں پاتھ پلاننگ کے لیے موثر ہے۔ یہ بے ترتیب، قابل رسائی کنفیگریشنز کا ایک درخت بنا کر کام کرتا ہے جب تک کہ ہدف تک کا راستہ نہ مل جائے۔

- **ٹاسک پلاننگ**:
    - یہ منصوبہ بندی کی ایک اعلیٰ سطح ہے۔ مثال کے طور پر، اگر کوئی صارف کہتا ہے، "مجھے کچن سے ایک ڈرنک لا دو،" تو روبوٹ کو پہلے کاموں کا ایک سلسلہ منصوبہ بندی کرنا چاہیے: 1. کچن میں نیویگیٹ کریں۔ 2. ریفریجریٹر کھولیں۔ 3. ڈرنک کی شناخت کریں۔ 4. ڈرنک پکڑیں۔ 5. صارف کے پاس واپس نیویگیٹ کریں۔
    - **بڑے لسانی ماڈلز (LLMs)** ٹاسک پلاننگ کے لیے ایک طاقتور ٹول کے طور پر ابھر رہے ہیں۔ وہ قدرتی زبان کے کمانڈز کو پارس کر سکتے ہیں اور اپنے عام فہم علم کا استعمال کرکے روبوٹ کو عمل درآمد کرنے کے لیے ایک قابل عمل سلسلہ پیدا کر سکتے ہیں۔

## 4.3 عملی اطلاقات اور مثالیں

### 4.3.1 خود چلانے والی کاریں
خود مختار گاڑیاں روبوٹک سسٹمز کی سب سے پیچیدہ حقیقی دنیا کی مثالوں میں سے ایک ہیں۔ وہ AI اور ML پر بہت زیادہ انحصار کرتی ہیں:
- **ادراک**: دنیا کا 360 ڈگری منظر بنانے اور دیگر گاڑیوں، پیدل چلنے والوں، اور ٹریفک لینز کا پتہ لگانے کے لیے کیمروں، LiDAR، اور ریڈار سے ڈیٹا کو فیوز کرنا۔
- **پیش گوئی**: سڑک پر دیگر ایجنٹوں کے مستقبل کے رویے کی پیش گوئی کرنے کے لیے ML ماڈلز کا استعمال کرنا۔
- **منصوبہ بندی**: حقیقی وقت میں ڈرائیونگ کے فیصلے کرنا، جیسے کہ کب لین تبدیل کرنی ہے، تیز کرنا ہے، یا بریک لگانی ہے۔

### 4.3.2 روبوٹک گرفت
روبوٹ کو متنوع اشیاء کو قابل اعتماد طریقے سے پکڑنا سکھانا ایک کلاسک روبوٹکس مسئلہ ہے۔ گہری کمک سیکھنا یہاں کامیابی سے لاگو کیا گیا ہے۔ مثال کے طور پر، ایک کیمرے والا روبوٹ بازو سمولیشن میں ہزاروں گھنٹے گزار سکتا ہے، بے ترتیب طور پر پیدا کردہ اشیاء کو اٹھانے کی کوشش کرتا ہے۔ کامیاب گرفت کے لیے انعام پا کر، وہ ایک مضبوط پالیسی سیکھتا ہے جو حقیقی دنیا کی ان اشیاء کو اٹھانے کے لیے عمومی ہو سکتی ہے جنہیں اس نے پہلے کبھی نہیں دیکھا۔

## 4.4 عملی مشقیں

### مشق 4.1: ایک انعام فنکشن ڈیزائن کرنا
آپ کو کمک سیکھنے کا استعمال کرتے ہوئے ایک ہیومنائڈ روبوٹ کو چلنا سکھانے کا کام سونپا گیا ہے۔ آپ کا مقصد ایک ایسا انعام فنکشن ڈیزائن کرنا ہے جو ایک مستحکم، آگے بڑھنے والی چال کی حوصلہ افزائی کرے۔ اپنے انعام فنکشن کے کم از کم تین اجزاء بیان کریں۔ ہر جزو کے لیے، یہ بتائیں کہ آیا یہ ایک انعام (مثبت) ہے یا سزا (منفی) اور وضاحت کریں کہ یہ کس رویے کی حوصلہ افزائی کرتا ہے۔
*مثال جزو: دھڑ کی آگے کی رفتار کے لیے ایک مثبت انعام۔*

### مشق 4.2: پاتھ پلاننگ پزل
ایک سادہ 8x8 گرڈ پر، ایک 'اسٹارٹ' سیل، ایک 'گول' سیل، اور کئی 'رکاوٹ' سیلز کو نشان زد کریں۔ اس راستے کو دستی طور پر ٹریس کریں جو A* الگورتھم اسٹارٹ سے گول تک تلاش کرے گا۔ آپ یہ ہر سیل تک پہنچنے کی "لاگت" پر نظر رکھ کر اور ہمیشہ سب سے کم لاگت والے سیل کو پھیلا کر کر سکتے ہیں۔

## 4.5 پروگرامنگ لیب

### 4.5.1 ماحول کو ترتیب دینا
یقینی بنائیں کہ آپ کے پاس Python اور `pybullet` انسٹال ہیں۔ اس لیب کے لیے، ہم عددی کارروائیوں کے لیے `numpy` بھی استعمال کریں گے۔
```bash
pip install pybullet numpy
```

### 4.5.2 کوڈ کا ٹکڑا: PyBullet میں A* کے ساتھ ایک راستے کا تصور
یہ لیب ایک اعلیٰ سطحی منصوبہ بندی الگورتھم (جیسے A*) اور ایک روبوٹ سمولیشن کے درمیان ایک تصوراتی ربط کو ظاہر کرتی ہے۔ ہم خود A* الگورتھم کو نافذ نہیں کریں گے لیکن یہ دکھائیں گے کہ آپ اس کے آؤٹ پٹ (وی پوائنٹس کا ایک سلسلہ) کو کیسے لیں گے اور PyBullet میں ایک سادہ روبوٹ کو اس پر عمل کرنے کا حکم دیں گے۔

```python
import pybullet as p
import pybullet_data
import time
import numpy as np

# --- A* Path (Conceptual) ---
# In a real scenario, an A* algorithm would generate this path based on a map.
# Here, we pre-define a simple path of (x, y) waypoints.
path_waypoints = [
    (2, 2), (2, 4), (2, 6), (4, 6), (6, 6), (6, 4), (6, 2), (4, 2), (2, 2)
]

# --- Simulation Setup ---
p.connect(p.GUI)
p.setAdditionalSearchPath(pybullet_data.getDataPath())
p.setGravity(0, 0, -9.81)
p.loadURDF("plane.urdf")

# Load a simple mobile robot (a sphere that we can move directly)
# This simplifies the problem by removing complex joint control
robotId = p.loadURDF("sphere2.urdf", [0, 0, 0.5], useFixedBase=False)

# --- Path Following Logic ---
# This loop will iterate through the waypoints and move the robot
for target_pos_2d in path_waypoints:
    target_pos = [target_pos_2d[0], target_pos_2d[1], 0.5]
    print(f"Moving to next waypoint: {target_pos}")

    current_pos, _ = p.getBasePositionAndOrientation(robotId)
    distance = np.linalg.norm(np.array(target_pos) - np.array(current_pos))

    # Move towards the target until we are close enough
    while distance > 0.1:
        # Calculate velocity command to move towards the target
        # This is a simple proportional controller for position
        direction = np.array(target_pos) - np.array(current_pos)
        direction = direction / np.linalg.norm(direction) # Normalize
        velocity = direction * 2.0  # Move at 2 m/s

        # PyBullet's resetBaseVelocity works well for simple kinematic control
        p.resetBaseVelocity(robotId, linearVelocity=[velocity[0], velocity[1], 0])

        # Step simulation and update our position
        p.stepSimulation()
        time.sleep(1./240.)
        current_pos, _ = p.getBasePositionAndOrientation(robotId)
        distance = np.linalg.norm(np.array(target_pos) - np.array(current_pos))

    print("Waypoint reached.")

# Stop the robot at the end
p.resetBaseVelocity(robotId, linearVelocity=[0, 0, 0])
print("\nPath following complete.")
time.sleep(5)
p.disconnect()
```
**وضاحت:**
- ہم پہلے ایک سادہ راستے کو 2D وی پوائنٹس کی ایک فہرست کے طور پر بیان کرتے ہیں۔ یہ راستہ وہ ہے جو A* جیسا پاتھ پلاننگ الگورتھم آؤٹ پٹ کے طور پر فراہم کرے گا۔
- ایک پیچیدہ ہیومنائڈ کے بجائے، ہم اپنے روبوٹ کی نمائندگی کرنے کے لیے ایک سادہ کرہ استعمال کرتے ہیں۔ یہ ہمیں پاتھ فالوونگ منطق پر توجہ مرکوز کرنے کی اجازت دیتا ہے۔
- لیب کا بنیادی حصہ ایک `while` لوپ ہے جو ایک سادہ تناسبی (P) کنٹرولر کو نافذ کرتا ہے۔ یہ مسلسل ٹارگٹ وی پوائنٹ کی سمت کا حساب لگاتا ہے اور روبوٹ کی رفتار کو اس سمت میں حرکت کرنے کے لیے سیٹ کرتا ہے۔
- `p.resetBaseVelocity()` روبوٹ کی رفتار کو براہ راست کنٹرول کرنے کے لیے استعمال کیا جاتا ہے، جو جوائنٹ لیول موٹر کنٹرول کی پیچیدگیوں کو دور کرتا ہے۔

## 4.6 باب کا خلاصہ
اس باب نے طبعی روبوٹ کو اس کے "دماغ" سے جوڑا، یہ دریافت کیا کہ AI اور مشین لرننگ روبوٹس کو سمجھنے، سیکھنے، اور منصوبہ بندی کرنے کی صلاحیت کیسے عطا کرتے ہیں۔ ہم نے کمپیوٹر ویژن اور پوائنٹ کلاؤڈ پروسیسنگ کے ذریعے ادراک کا احاطہ کیا، جو روبوٹس کو اپنے ماحول کے ماڈل بنانے کی اجازت دیتا ہے۔ ہم نے سیکھنے کے نمونوں کی گہرائی میں غوطہ لگایا، تقلید سیکھنے کا موازنہ کمک سیکھنے کے طاقتور آزمائش اور غلطی کے نقطہ نظر سے کیا۔ آخر میں، ہم نے جانچا کہ پاتھ اور ٹاسک پلاننگ الگورتھمز روبوٹس کو اعلیٰ سطحی اہداف کو قابل عمل مراحل میں توڑنے کے قابل کیسے بناتے ہیں۔ یہ AI/ML تکنیکیں ہیں جو ایک روبوٹ کو ایک پہلے سے پروگرام شدہ مشین سے ایک خود مختار ایجنٹ تک بلند کرتی ہیں جو حقیقی دنیا کی پیچیدگی میں کام کرنے کی صلاحیت رکھتا ہے۔

## 4.7 مزید مطالعہ
-   **کتابیں**:
    -   `ڈیپ لرننگ` از ایان گڈفیلو، یوشوا بینجیو، اور ایرون کورویل۔
    -   `کمک سیکھنا: ایک تعارف` از رچرڈ ایس۔ سوٹن اور اینڈریو جی۔ بارٹو۔
    -   `کمپیوٹر ویژن: الگورتھمز اور ایپلی کیشنز` از رچرڈ سزیلیسکی۔
-   **آن لائن کورسز**:
    -   کورسیرا: "ڈیپ لرننگ سپیشلائزیشن" از اینڈریو این جی۔
    -   یوڈاسٹی: "کمک سیکھنا نینو ڈگری۔"
